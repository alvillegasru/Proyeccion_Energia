{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For -example, here's several helpful packages to load\nimport os\n\n# Limitar JAX a un único hilo/dispositivo\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=1\"\nos.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"  # Asegúrate de que usa CPU si no necesitas GPU.\nimport multiprocessing\n\nif __name__ == \"__main__\":\n    multiprocessing.set_start_method(\"spawn\", force=True)\n!pip install --upgrade jax jaxlib\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pytorch-tabnet --quiet\n!pip install wget --quiet\n!pip install pytorch-tabnet optuna\nfrom pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\nimport os\nimport wget\nfrom typing import Optional, Union, Tuple\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\ndef get_labels(x: pd.Series) -> pd.Series:\n    labels, _ = pd.factorize(x)\n    return pd.Series(labels, name=x.name, index=x.index)\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport torch\nfrom pytorch_tabnet.augmentations import RegressionSMOTE\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import QuantileTransformer\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport optuna\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, QuantileTransformer\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.datasets import fetch_openml\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom itertools import filterfalse\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport torch\nimport optuna\nimport warnings\n\n# 1. Deshabilitar el logging de Optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# 2. Suprimir advertencias específicas\nwarnings.filterwarnings(\"ignore\", category=UserWarning)  # Para advertencias de PyTorch TabNet o de otros módulos\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)  # Para advertencias futuras como 'suggest_loguniform'\n\ndef my_mse_loss_fn(y_pred, y_true):\n    \"\"\"\n    Dummy example similar to using default torch.nn.functional.mse_loss\n    \"\"\"\n    mse_loss = (y_true - y_pred) ** 2\n    return torch.mean(mse_loss)\ndef my_rmse_loss_fn(y_pred, y_true):\n    \"\"\"\n    Función de pérdida que calcula el RMSE entre la predicción y el valor verdadero.\n    \"\"\"\n    mse_loss = (y_true - y_pred) ** 2\n    mean_mse_loss = torch.mean(mse_loss)\n    rmse_loss = torch.sqrt(mean_mse_loss)\n    return rmse_loss\ndef my_mae_loss_fn(y_pred, y_true):\n    \"\"\"\n    Calcula el error absoluto medio (Mean Absolute Error, MAE) entre las predicciones y los valores reales.\n    Similar al uso de torch.nn.functional.l1_loss.\n    \"\"\"\n    mae_loss = torch.abs(y_true - y_pred)\n    return torch.mean(mae_loss)\ndef my_mape_loss_fn(y_pred, y_true):\n    \"\"\"\n    Calcula el error porcentual absoluto medio (Mean Absolute Percentage Error, MAPE).\n    \"\"\"\n    epsilon = 1e-8  # Evita divisiones por cero\n    percentage_error = torch.abs((y_true - y_pred) / (y_true + epsilon))\n    mape_loss = torch.mean(percentage_error) * 100\n    return mape_loss\n\ndef my_r2_score_fn(y_pred, y_true):\n    \"\"\"\n    Calcula el coeficiente de determinación R².\n    \"\"\"\n    total_variance = torch.var(y_true, unbiased=False)  # Varianza total\n    unexplained_variance = torch.mean((y_true - y_pred) ** 2)  # Varianza no explicada (MSE)\n    r2_score = 1 - (unexplained_variance / total_variance)\n    return 1-r2_score\ndef my_log_cosh_loss_fn(y_pred, y_true):\n    \"\"\"\n    Calcula la pérdida log-cosh.\n    \"\"\"\n    log_cosh_loss = torch.log(torch.cosh(y_true - y_pred))\n    return torch.mean(log_cosh_loss)\n\ndef my_huber_loss_fn(y_pred, y_true, delta=1.0):\n    \"\"\"\n    Calcula la pérdida Huber.\n    \"\"\"\n    abs_error = torch.abs(y_true - y_pred)\n    quadratic = torch.minimum(abs_error, torch.tensor(delta))\n    linear = abs_error - quadratic\n    huber_loss = 0.5 * quadratic ** 2 + delta * linear\n    return torch.mean(huber_loss)\n\ndef my_msle_loss_fn(y_pred, y_true):\n    \"\"\"\n    Calcula el error cuadrático medio logarítmico (MSLE).\n    \"\"\"\n    epsilon = 1e-8  # Evita logaritmos de cero\n    log_diff = torch.log1p(y_pred + epsilon) - torch.log1p(y_true + epsilon)\n    msle_loss = torch.mean(log_diff ** 2)\n    return msle_loss\n\nimport torch\nimport torch.nn as nn\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nclass CustomTabNetRegressor(TabNetRegressor):\n    def __init__(self, *args, **kwargs):\n        super(CustomTabNetRegressor, self).__init__(*args, **kwargs)\n        self.modified = False\n\n    def fit(self, X_train, y_train, *args, **kwargs):\n        super(CustomTabNetRegressor, self).fit(X_train, y_train, *args, **kwargs)\n        if not self.modified:\n            # Modificar la capa de salida para incluir ReLU\n            original_forward = self.network.forward\n\n            def modified_forward(inputs):\n                # Obtener la salida original de la red\n                output, M_loss = original_forward(inputs)\n                # Aplicar ReLU a la salida\n                output = torch.relu(output)\n                return output, M_loss\n\n            # Reemplazar el método forward de la red\n            self.network.forward = modified_forward\n            self.modified = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/alvillegasru/Proyeccion_Energia.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nenergias = pd.read_parquet('/kaggle/working/Proyeccion_Energia/Databases/DF_TRF.parquet')\nenergias = energias[energias['CTO'] != 'JTG23L12'].reset_index(drop=True)\n\nenergias['KWH_M'] = energias['KWH_M'].astype(float)\nenergias['SUB'] = energias['CTO'].str[:3]\nenergias_13_2 = energias[energias['CTO'].str[3:5]=='23']\n\nenergias_MAN = energias_13_2[energias_13_2['SUB'].isin(['MAN','PSO','CHI','ENE','AZA'])].reset_index(drop=True)\n\nenergias_ordenado = energias_MAN.sort_values(by=['ANIO','MES','CTO']).reset_index(drop=True)\nenergias_ordenado['FECHA']=pd.to_datetime(energias_ordenado['ANIO'].astype(str)+'-'+energias_ordenado['MES'].astype(str)+'-01',format='%Y-%m-%d')\nenergias_ordenado.drop(columns=['ANIO','MES','SUB'],inplace=True)\n\nclientes = pd.read_parquet('/kaggle/working/Proyeccion_Energia/Databases/Crecimiento Poblacional/Cantidad de clientes promedio transformador.parquet')\n\nclientes['PROMEDIO_CLIENTES']=np.ceil(clientes['PROMEDIO_CLIENTES']).astype(int)\n\nclientes['FECHA']=pd.to_datetime(clientes['AÑO'].astype(str)+'-'+clientes['MES'].astype(str)+'-01',format='%Y-%m-%d')\nclientes.drop(columns=['AÑO','MES'],inplace=True)\n\ndf_merged = pd.merge(energias_ordenado, clientes, left_on=['TF','FECHA','CTO'],right_on=['TPARENT','FECHA','FPARENT'], how='left')\ndf_merged.drop(columns=['TPARENT','FPARENT'],inplace=True)\n\nubicacion= pd.read_csv('/kaggle/working/Proyeccion_Energia/Databases/Crecimiento Poblacional/Trafo_Estrato_Coordenada.csv',sep=';')\n\nubicacion.drop(columns=['MAX_ESTRATO'],inplace=True)\n\ndf_merged2=pd.merge(df_merged,ubicacion,left_on=['TF','CTO'],right_on=['CODIGO','CIRCUITO'],how='left')\ndf_merged2.drop(columns=['CODIGO'],inplace=True)\n\ndf_no_duplicados = df_merged2.drop_duplicates(subset=['TF', 'CTO'])\n\ndf_merged2.dropna(inplace=True)\n\ndf_merged2['SUB'] = df_merged2['CIRCUITO'].str[:5]\n\nif 'Bahia' in df_merged2.columns:\n    df_merged2.drop(columns=['Bahia'], inplace=True)\n\ntarget='KWH_M'\n\nXdata = df_merged2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Xdata[\"FECHA\"]=Xdata[\"FECHA\"].astype(\"int64\")\nXdata.drop(['FECHA'],axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y1 = Xdata[target].values.reshape(-1, 1)\n\nXdata.drop(target, axis=1, inplace=True)\ndf = Xdata.copy()\n\n# Identificar columnas numéricas y categóricas\nNUMERIC_COLUMNS = df.select_dtypes(include=['number']).columns.tolist()\nCATEGORICAL_COLUMNS = df.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# Rellenar valores faltantes\n# Para numéricos: valor muy alto (3 veces el máximo de cada columna)\n# Para categóricos: \"no aplica\"\nfor col in NUMERIC_COLUMNS:\n    max_value = df[col].max()\n    df[col].fillna(-10 * max_value, inplace=True)\n\nfor col in CATEGORICAL_COLUMNS:\n    df[col].fillna(\"no aplica\", inplace=True)\n\n# Codificar variables categóricas y obtener sus dimensiones\ncategorical_dims = {}\nfor col in CATEGORICAL_COLUMNS:\n    l_enc = LabelEncoder()\n    df[col] = l_enc.fit_transform(df[col].astype(str))\n    categorical_dims[col] = len(l_enc.classes_)\n\n# Crear lista de características\nunused_feat = []\nfeatures = [col for col in df.columns if col not in unused_feat + [target]]\n\n# Obtener índices y dimensiones de variables categóricas\n\n\n# Preparar datos\nX = df[features].values.astype('float32')\ny = y1.astype('float32')\npercentiles = np.percentile(y, [33.33, 66.66])  # Divisiones en tercios\ny_categorized = np.digitize(y[:,0:1].flatten(), bins=percentiles).astype(int)\n\n# Escalar la variable objetivo\nscaler =  MinMaxScaler()\ny_scaled = scaler.fit_transform(y)\n\n# Dividir los datos\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_scaled, test_size=0.2, random_state=42,stratify=y_categorized)\npercentiles = np.percentile(y, [33.33, 66.66])  # Divisiones en tercios\ny_categorized_t = np.digitize(y_train[:,0:1].flatten(), bins=percentiles).astype(int)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42,stratify=y_categorized_t)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\ncat_dims = [categorical_dims[f] for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\ncat_emb_dim = [min(4, (dim + 1) // 2) for dim in cat_dims]\nprint(cat_idxs,cat_dims,cat_emb_dim,)    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def objective_regression(trial):\n#     # Hiperparámetros ajustados\n#     n_d = trial.suggest_int('n_d', 8, 128)\n#     n_a = trial.suggest_int('n_a', 8, 128)\n#     n_steps = trial.suggest_int('n_steps', 3, 10)\n#     gamma = trial.suggest_loguniform('gamma', 1e-2, 10.0)\n#     lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-5, 1e-2)\n#     batch_size = trial.suggest_categorical('batch_size', [256, 512])\n#     mask_type = trial.suggest_categorical('mask_type', ['entmax', 'sparsemax'])\n#     emb = trial.suggest_int('emb', 3, 10)\n    \n#     momentum = trial.suggest_float('momentum', 0.1, 0.9)\n#     learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n#     weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n#     scheduler_gamma = trial.suggest_float('scheduler_gamma', 0.7, 0.95)\n#     step_size = trial.suggest_int('step_size', 5, 15)\n#     virtual_batch_size = trial.suggest_categorical('virtual_batch_size', [64, 128])\n    \n#     optimizer_type = trial.suggest_categorical('optimizer_type', ['adam', 'sgd', 'rmsprop'])\n\n#     # Configurar optimizador\n#     if optimizer_type == 'adam':\n#         optimizer_fn = torch.optim.Adam\n#         optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n#     elif optimizer_type == 'sgd':\n#         optimizer_fn = torch.optim.SGD\n#         optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n#     elif optimizer_type == 'rmsprop':\n#         optimizer_fn = torch.optim.RMSprop\n#         optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n\n#     # Incorporar valores categóricos\n#     cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n#     cat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\n#     cat_emb_dim = [min(emb, (dim + 1) // 2) for dim in cat_dims]\n    \n#     # Augmentations\n#     p = trial.suggest_float('p', 0.1, 0.9)\n#     aug = RegressionSMOTE(p=p)\n\n#     # Crear modelo TabNet\n#     model = TabNetRegressor(\n#         cat_dims=cat_dims,\n#         cat_emb_dim=cat_emb_dim,\n#         cat_idxs=cat_idxs,\n#         n_d=n_d,\n#         n_a=n_a,\n#         n_steps=n_steps,\n#         gamma=gamma,\n#         lambda_sparse=lambda_sparse,\n#         mask_type=mask_type,\n#         optimizer_fn=optimizer_fn,\n#         optimizer_params=optimizer_params,\n#         scheduler_params={\"gamma\": scheduler_gamma, \"step_size\": step_size},\n#         scheduler_fn=torch.optim.lr_scheduler.StepLR,\n#         verbose=False,\n#     )\n    \n#     # Entrenar el modelo\n#     model.fit(\n#         X_train=X_train,\n#         y_train=y_train,\n#         eval_set=[(X_train, y_train), (X_valid, y_valid)],\n#         eval_name=['train', 'valid'],\n#         eval_metric=['mae'],\n#         loss_fn=my_r2_score_fn,\n#         max_epochs=50,\n#         patience=40,\n#         batch_size=batch_size,\n#         virtual_batch_size=virtual_batch_size,\n#         num_workers=1,\n#         drop_last=False,\n#         augmentations=aug,\n#     )\n\n#     # Evaluación    \n#     mae = model.history['loss'][-1]\n#     return mae\n\n# # Crear el estudio\n# study_class = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n# study_class.optimize(objective_regression, n_trials=10)\n\n# # Resultados\n# print(\"Best hyperparameters for regression: \", study_class.best_params)\n# print(\"Best mae: \", study_class.best_value)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hip= {'n_d': 36, 'n_a': 26, 'n_steps': 9, 'gamma': 0.036821698755764784, 'lambda_sparse': 1.9211354881041664e-05, 'batch_size': 512, 'mask_type': 'sparsemax', 'emb': 9, 'momentum': 0.30846861891333355, 'learning_rate': 0.002056760319661826, 'weight_decay': 2.4601333519135903e-05, 'scheduler_gamma': 0.7634440567538022, 'step_size': 14, 'virtual_batch_size': 64, 'optimizer_type': 'rmsprop', 'p': 0.19153546053382878}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parámetros obtenidos del estudio\n# Parámetros obtenidos del estudio\nbest_params =  hip\n\n# Asignación de parámetros\nn_d = best_params['n_d']\nn_a = best_params['n_a']\nn_steps = best_params['n_steps']\ngamma = best_params['gamma']\nlambda_sparse = best_params['lambda_sparse']\nmask_type = best_params['mask_type']\nbatch_size = best_params['batch_size']\nemb = best_params['emb']\np = best_params['p']\nmomentum = best_params['momentum']\nlearning_rate = best_params['learning_rate']\nweight_decay = best_params['weight_decay']\nscheduler_gamma = best_params['scheduler_gamma']\nstep_size = best_params['step_size']\nvirtual_batch_size = best_params['virtual_batch_size']\noptimizer_type = best_params['optimizer_type']\n\nif optimizer_type == 'adam':\n    optimizer_fn = torch.optim.Adam\n    optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\nelif optimizer_type == 'adamw':\n    optimizer_fn = torch.optim.AdamW\n    optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\nelif optimizer_type == 'sgd':\n    optimizer_fn = torch.optim.SGD\n    optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\nelif optimizer_type == 'rmsprop':\n    optimizer_fn = torch.optim.RMSprop\n    optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n\n# Definición de la técnica de aumento de datos\naug = RegressionSMOTE(p=p)\n\n# Identificación de columnas categóricas y sus dimensiones\ncat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\ncat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\ncat_emb_dim = [min(emb, (dim + 1) // 2) for dim in cat_dims]\nimport torch\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Valores de lambda_sparse para iterar\nlambda_values = [lambda_sparse]\n\n# Listas para guardar las máscaras y matrices de explicación de cada iteración\nall_masks = []\nall_explain_matrices = []\n\nfor iteration, lambda_sparse in enumerate(lambda_values):\n    print(f\"Iteración {iteration + 1}, lambda_sparse={lambda_sparse}\")\n    \n    # Inicialización del modelo TabNetRegressor\n    clf = TabNetRegressor(\n        cat_dims=cat_dims,\n        cat_emb_dim=cat_emb_dim,\n        cat_idxs=cat_idxs,\n        n_d=n_d,\n        n_a=n_a,\n        n_steps=n_steps,\n        gamma=gamma,\n        lambda_sparse=lambda_sparse,\n        mask_type=mask_type,\n        optimizer_fn=optimizer_fn,\n        optimizer_params=optimizer_params,\n        scheduler_params={\"gamma\": scheduler_gamma, \"step_size\": step_size},\n        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n        momentum=momentum,\n        verbose=True\n    )\n\n    # Entrenar el modelo\n    clf.fit(\n        X_train=X_train,\n        y_train=y_train[:,0:1],\n        eval_set=[(X_train, y_train[:,0:1]), (X_valid, y_valid[:,0:1])],\n        eval_name=['train', 'valid'],\n        eval_metric=['mae'],\n        loss_fn=my_r2_score_fn,\n        max_epochs=200,\n        patience=60,\n        batch_size=batch_size,\n        virtual_batch_size=virtual_batch_size,\n        num_workers=0,\n        drop_last=False,\n        augmentations=aug,\n    )\n\n#     # Generar explicación\n#     explain_matrix, masks = clf.explain(X, normalize=True)\n\n#     # Guardar resultados\n#     all_masks.append(np.array([masks[i] for i in range(n_steps)]))  # Guardar las máscaras\n#     all_explain_matrices.append(explain_matrix)  # Guardar explain_matrix como array de NumPy\n# savepath='/kaggle/working/'\n# # Guardar las máscaras y matrices de explicación en archivos\n# for i, (masks, explain_matrix) in enumerate(zip(all_masks, all_explain_matrices)):\n#     # Guardar las máscaras como un archivo numpy\n#     np.save(f\"{savepath}masks_iteration_{i+1}.npy\", masks, allow_pickle=True)\n#     # Guardar la matriz de explicación como un archivo numpy\n#     np.save(f\"{savepath}explain_matrix_iteration_{i+1}.npy\", explain_matrix,allow_pickle=True)\n\n# print(\"¡Proceso completo! Las máscaras y matrices de explicación se han guardado correctamente.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ny_pred=clf.predict(X_test)\n# Calcular el R2\nr2 = r2_score(y_test, y_pred)\nplt.figure(figsize=(8,6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.xlabel('True Values')\nplt.ylabel('Predicted Values')\nplt.title('Regression Output 1: True vs Predicted')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r-', label=f'($R^2={r2:.2f}$)')\n# Mostrar leyenda y gráfico\nplt.yscale('symlog')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"explain_matrix, masks = clf.explain(X, normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min_val = np.min([np.min(masks[i]) for i in range(n_steps)])\nmax_val = np.max([np.max(masks[i]) for i in range(n_steps)])\nfig, axs = plt.subplots(1, n_steps, figsize=(20,20))\n# Iteración para mostrar cada máscara\nfor i in range(n_steps):\n    im = axs[i].imshow(masks[i], aspect='auto', cmap='viridis', vmin=min_val, vmax=max_val)\n    axs[i].set_title(f\"Mask {i + 1}\", fontsize=16)\n    axs[i].set_xlabel(\"Features\", fontsize=14)\n    if i == 0:  # Solo mostrar ejes en la primera subfigura\n        axs[i].set_ylabel(\"Samples\", fontsize=14)\n    else:\n        axs[i].set_yticks([])\n# Añadir un único colorbar para toda la figura al final\ncbar = fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.046, pad=-0.22)\ncbar.set_label(\"Values\", fontsize=14)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(22, 15))\npd.Series(clf.feature_importances_, index=df.columns).plot.bar(title='Global Feature Importances')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}