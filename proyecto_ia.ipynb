{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30804,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvillegasru/Proyeccion_Energia/blob/main/proyecto_ia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For -example, here's several helpful packages to load\n",
        "import os\n",
        "\n",
        "# Limitar JAX a un único hilo/dispositivo\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=1\"\n",
        "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"  # Asegúrate de que usa CPU si no necesitas GPU.\n",
        "import multiprocessing\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.set_start_method(\"spawn\", force=True)\n",
        "!pip install --upgrade jax jaxlib\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "seZdy8JisEqc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet --quiet\n",
        "!pip install wget --quiet\n",
        "!pip install pytorch-tabnet optuna\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import os\n",
        "import wget\n",
        "from typing import Optional, Union, Tuple\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "def get_labels(x: pd.Series) -> pd.Series:\n",
        "    labels, _ = pd.factorize(x)\n",
        "    return pd.Series(labels, name=x.name, index=x.index)\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "from pytorch_tabnet.augmentations import RegressionSMOTE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import optuna\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import filterfalse\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "import optuna\n",
        "import warnings\n",
        "\n",
        "# 1. Deshabilitar el logging de Optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# 2. Suprimir advertencias específicas\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Para advertencias de PyTorch TabNet o de otros módulos\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Para advertencias futuras como 'suggest_loguniform'\n",
        "\n",
        "def my_mse_loss_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Dummy example similar to using default torch.nn.functional.mse_loss\n",
        "    \"\"\"\n",
        "    mse_loss = (y_true - y_pred) ** 2\n",
        "    return torch.mean(mse_loss)\n",
        "def my_rmse_loss_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Función de pérdida que calcula el RMSE entre la predicción y el valor verdadero.\n",
        "    \"\"\"\n",
        "    mse_loss = (y_true - y_pred) ** 2\n",
        "    mean_mse_loss = torch.mean(mse_loss)\n",
        "    rmse_loss = torch.sqrt(mean_mse_loss)\n",
        "    return rmse_loss\n",
        "def my_mae_loss_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calcula el error absoluto medio (Mean Absolute Error, MAE) entre las predicciones y los valores reales.\n",
        "    Similar al uso de torch.nn.functional.l1_loss.\n",
        "    \"\"\"\n",
        "    mae_loss = torch.abs(y_true - y_pred)\n",
        "    return torch.mean(mae_loss)\n",
        "def my_mape_loss_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calcula el error porcentual absoluto medio (Mean Absolute Percentage Error, MAPE).\n",
        "    \"\"\"\n",
        "    epsilon = 1e-8  # Evita divisiones por cero\n",
        "    percentage_error = torch.abs((y_true - y_pred) / (y_true + epsilon))\n",
        "    mape_loss = torch.mean(percentage_error) * 100\n",
        "    return mape_loss\n",
        "\n",
        "def my_r2_score_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calcula el coeficiente de determinación R².\n",
        "    \"\"\"\n",
        "    total_variance = torch.var(y_true, unbiased=False)  # Varianza total\n",
        "    unexplained_variance = torch.mean((y_true - y_pred) ** 2)  # Varianza no explicada (MSE)\n",
        "    r2_score = 1 - (unexplained_variance / total_variance)\n",
        "    return 1-r2_score\n",
        "def my_log_cosh_loss_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calcula la pérdida log-cosh.\n",
        "    \"\"\"\n",
        "    log_cosh_loss = torch.log(torch.cosh(y_true - y_pred))\n",
        "    return torch.mean(log_cosh_loss)\n",
        "\n",
        "def my_huber_loss_fn(y_pred, y_true, delta=1.0):\n",
        "    \"\"\"\n",
        "    Calcula la pérdida Huber.\n",
        "    \"\"\"\n",
        "    abs_error = torch.abs(y_true - y_pred)\n",
        "    quadratic = torch.minimum(abs_error, torch.tensor(delta))\n",
        "    linear = abs_error - quadratic\n",
        "    huber_loss = 0.5 * quadratic ** 2 + delta * linear\n",
        "    return torch.mean(huber_loss)\n",
        "\n",
        "def my_msle_loss_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calcula el error cuadrático medio logarítmico (MSLE).\n",
        "    \"\"\"\n",
        "    epsilon = 1e-8  # Evita logaritmos de cero\n",
        "    log_diff = torch.log1p(y_pred + epsilon) - torch.log1p(y_true + epsilon)\n",
        "    msle_loss = torch.mean(log_diff ** 2)\n",
        "    return msle_loss\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "class CustomTabNetRegressor(TabNetRegressor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomTabNetRegressor, self).__init__(*args, **kwargs)\n",
        "        self.modified = False\n",
        "\n",
        "    def fit(self, X_train, y_train, *args, **kwargs):\n",
        "        super(CustomTabNetRegressor, self).fit(X_train, y_train, *args, **kwargs)\n",
        "        if not self.modified:\n",
        "            # Modificar la capa de salida para incluir ReLU\n",
        "            original_forward = self.network.forward\n",
        "\n",
        "            def modified_forward(inputs):\n",
        "                # Obtener la salida original de la red\n",
        "                output, M_loss = original_forward(inputs)\n",
        "                # Aplicar ReLU a la salida\n",
        "                output = torch.relu(output)\n",
        "                return output, M_loss\n",
        "\n",
        "            # Reemplazar el método forward de la red\n",
        "            self.network.forward = modified_forward\n",
        "            self.modified = True\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "nYxmY_h4sEqe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alvillegasru/Proyeccion_Energia.git"
      ],
      "metadata": {
        "trusted": true,
        "id": "4VQ9ntvLsEqe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "energias = pd.read_parquet('/kaggle/working/Proyeccion_Energia/Databases/DF_TRF.parquet')\n",
        "energias = energias[energias['CTO'] != 'JTG23L12'].reset_index(drop=True)\n",
        "\n",
        "energias['KWH_M'] = energias['KWH_M'].astype(float)\n",
        "energias['SUB'] = energias['CTO'].str[:3]\n",
        "energias_13_2 = energias[energias['CTO'].str[3:5]=='23']\n",
        "\n",
        "energias_MAN = energias_13_2[energias_13_2['SUB'].isin(['MAN','PSO','CHI','ENE','AZA'])].reset_index(drop=True)\n",
        "\n",
        "energias_ordenado = energias_MAN.sort_values(by=['ANIO','MES','CTO']).reset_index(drop=True)\n",
        "energias_ordenado['FECHA']=pd.to_datetime(energias_ordenado['ANIO'].astype(str)+'-'+energias_ordenado['MES'].astype(str)+'-01',format='%Y-%m-%d')\n",
        "energias_ordenado.drop(columns=['ANIO','MES','SUB'],inplace=True)\n",
        "\n",
        "clientes = pd.read_parquet('/kaggle/working/Proyeccion_Energia/Databases/Crecimiento Poblacional/Cantidad de clientes promedio transformador.parquet')\n",
        "\n",
        "clientes['PROMEDIO_CLIENTES']=np.ceil(clientes['PROMEDIO_CLIENTES']).astype(int)\n",
        "\n",
        "clientes['FECHA']=pd.to_datetime(clientes['AÑO'].astype(str)+'-'+clientes['MES'].astype(str)+'-01',format='%Y-%m-%d')\n",
        "clientes.drop(columns=['AÑO','MES'],inplace=True)\n",
        "\n",
        "df_merged = pd.merge(energias_ordenado, clientes, left_on=['TF','FECHA','CTO'],right_on=['TPARENT','FECHA','FPARENT'], how='left')\n",
        "df_merged.drop(columns=['TPARENT','FPARENT'],inplace=True)\n",
        "\n",
        "ubicacion= pd.read_csv('/kaggle/working/Proyeccion_Energia/Databases/Crecimiento Poblacional/Trafo_Estrato_Coordenada.csv',sep=';')\n",
        "\n",
        "ubicacion.drop(columns=['MAX_ESTRATO'],inplace=True)\n",
        "\n",
        "df_merged2=pd.merge(df_merged,ubicacion,left_on=['TF','CTO'],right_on=['CODIGO','CIRCUITO'],how='left')\n",
        "df_merged2.drop(columns=['CODIGO'],inplace=True)\n",
        "\n",
        "df_no_duplicados = df_merged2.drop_duplicates(subset=['TF', 'CTO'])\n",
        "\n",
        "df_merged2.dropna(inplace=True)\n",
        "\n",
        "df_merged2['SUB'] = df_merged2['CIRCUITO'].str[:5]\n",
        "\n",
        "if 'Bahia' in df_merged2.columns:\n",
        "    df_merged2.drop(columns=['Bahia'], inplace=True)\n",
        "\n",
        "target='KWH_M'\n",
        "\n",
        "Xdata = df_merged2\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "UNFMv5YssEqe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Xdata[\"FECHA\"]=Xdata[\"FECHA\"].astype(\"int64\")\n",
        "Xdata.drop(['FECHA'],axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CDfawtTfsEqe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y1 = Xdata[target].values.reshape(-1, 1)\n",
        "\n",
        "Xdata.drop(target, axis=1, inplace=True)\n",
        "df = Xdata.copy()\n",
        "\n",
        "# Identificar columnas numéricas y categóricas\n",
        "NUMERIC_COLUMNS = df.select_dtypes(include=['number']).columns.tolist()\n",
        "CATEGORICAL_COLUMNS = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Rellenar valores faltantes\n",
        "# Para numéricos: valor muy alto (3 veces el máximo de cada columna)\n",
        "# Para categóricos: \"no aplica\"\n",
        "for col in NUMERIC_COLUMNS:\n",
        "    max_value = df[col].max()\n",
        "    df[col].fillna(-10 * max_value, inplace=True)\n",
        "\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    df[col].fillna(\"no aplica\", inplace=True)\n",
        "\n",
        "# Codificar variables categóricas y obtener sus dimensiones\n",
        "categorical_dims = {}\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    l_enc = LabelEncoder()\n",
        "    df[col] = l_enc.fit_transform(df[col].astype(str))\n",
        "    categorical_dims[col] = len(l_enc.classes_)\n",
        "\n",
        "# Crear lista de características\n",
        "unused_feat = []\n",
        "features = [col for col in df.columns if col not in unused_feat + [target]]\n",
        "\n",
        "# Obtener índices y dimensiones de variables categóricas\n",
        "\n",
        "\n",
        "# Preparar datos\n",
        "X = df[features].values.astype('float32')\n",
        "y = y1.astype('float32')\n",
        "percentiles = np.percentile(y, [33.33, 66.66])  # Divisiones en tercios\n",
        "y_categorized = np.digitize(y[:,0:1].flatten(), bins=percentiles).astype(int)\n",
        "\n",
        "# Escalar la variable objetivo\n",
        "scaler =  MinMaxScaler()\n",
        "y_scaled = scaler.fit_transform(y)\n",
        "\n",
        "# Dividir los datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_scaled, test_size=0.2, random_state=42,stratify=y_categorized)\n",
        "percentiles = np.percentile(y, [33.33, 66.66])  # Divisiones en tercios\n",
        "y_categorized_t = np.digitize(y_train[:,0:1].flatten(), bins=percentiles).astype(int)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42,stratify=y_categorized_t)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-VLEvOeisEqe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "cat_dims = [categorical_dims[f] for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "cat_emb_dim = [min(4, (dim + 1) // 2) for dim in cat_dims]\n",
        "print(cat_idxs,cat_dims,cat_emb_dim,)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4vbl1HzosEqe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# def objective_regression(trial):\n",
        "#     # Hiperparámetros ajustados\n",
        "#     n_d = trial.suggest_int('n_d', 8, 128)\n",
        "#     n_a = trial.suggest_int('n_a', 8, 128)\n",
        "#     n_steps = trial.suggest_int('n_steps', 3, 10)\n",
        "#     gamma = trial.suggest_loguniform('gamma', 1e-2, 10.0)\n",
        "#     lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-5, 1e-2)\n",
        "#     batch_size = trial.suggest_categorical('batch_size', [256, 512])\n",
        "#     mask_type = trial.suggest_categorical('mask_type', ['entmax', 'sparsemax'])\n",
        "#     emb = trial.suggest_int('emb', 3, 10)\n",
        "\n",
        "#     momentum = trial.suggest_float('momentum', 0.1, 0.9)\n",
        "#     learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "#     weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
        "#     scheduler_gamma = trial.suggest_float('scheduler_gamma', 0.7, 0.95)\n",
        "#     step_size = trial.suggest_int('step_size', 5, 15)\n",
        "#     virtual_batch_size = trial.suggest_categorical('virtual_batch_size', [64, 128])\n",
        "\n",
        "#     optimizer_type = trial.suggest_categorical('optimizer_type', ['adam', 'sgd', 'rmsprop'])\n",
        "\n",
        "#     # Configurar optimizador\n",
        "#     if optimizer_type == 'adam':\n",
        "#         optimizer_fn = torch.optim.Adam\n",
        "#         optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n",
        "#     elif optimizer_type == 'sgd':\n",
        "#         optimizer_fn = torch.optim.SGD\n",
        "#         optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "#     elif optimizer_type == 'rmsprop':\n",
        "#         optimizer_fn = torch.optim.RMSprop\n",
        "#         optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "\n",
        "#     # Incorporar valores categóricos\n",
        "#     cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "#     cat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\n",
        "#     cat_emb_dim = [min(emb, (dim + 1) // 2) for dim in cat_dims]\n",
        "\n",
        "#     # Augmentations\n",
        "#     p = trial.suggest_float('p', 0.1, 0.9)\n",
        "#     aug = RegressionSMOTE(p=p)\n",
        "\n",
        "#     # Crear modelo TabNet\n",
        "#     model = TabNetRegressor(\n",
        "#         cat_dims=cat_dims,\n",
        "#         cat_emb_dim=cat_emb_dim,\n",
        "#         cat_idxs=cat_idxs,\n",
        "#         n_d=n_d,\n",
        "#         n_a=n_a,\n",
        "#         n_steps=n_steps,\n",
        "#         gamma=gamma,\n",
        "#         lambda_sparse=lambda_sparse,\n",
        "#         mask_type=mask_type,\n",
        "#         optimizer_fn=optimizer_fn,\n",
        "#         optimizer_params=optimizer_params,\n",
        "#         scheduler_params={\"gamma\": scheduler_gamma, \"step_size\": step_size},\n",
        "#         scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "#         verbose=False,\n",
        "#     )\n",
        "\n",
        "#     # Entrenar el modelo\n",
        "#     model.fit(\n",
        "#         X_train=X_train,\n",
        "#         y_train=y_train,\n",
        "#         eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "#         eval_name=['train', 'valid'],\n",
        "#         eval_metric=['mae'],\n",
        "#         loss_fn=my_r2_score_fn,\n",
        "#         max_epochs=50,\n",
        "#         patience=40,\n",
        "#         batch_size=batch_size,\n",
        "#         virtual_batch_size=virtual_batch_size,\n",
        "#         num_workers=1,\n",
        "#         drop_last=False,\n",
        "#         augmentations=aug,\n",
        "#     )\n",
        "\n",
        "#     # Evaluación\n",
        "#     mae = model.history['loss'][-1]\n",
        "#     return mae\n",
        "\n",
        "# # Crear el estudio\n",
        "# study_class = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
        "# study_class.optimize(objective_regression, n_trials=10)\n",
        "\n",
        "# # Resultados\n",
        "# print(\"Best hyperparameters for regression: \", study_class.best_params)\n",
        "# print(\"Best mae: \", study_class.best_value)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9e_r2D5isEqf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "hip= {'n_d': 36, 'n_a': 26, 'n_steps': 9, 'gamma': 0.036821698755764784, 'lambda_sparse': 1.9211354881041664e-05, 'batch_size': 512, 'mask_type': 'sparsemax', 'emb': 9, 'momentum': 0.30846861891333355, 'learning_rate': 0.002056760319661826, 'weight_decay': 2.4601333519135903e-05, 'scheduler_gamma': 0.7634440567538022, 'step_size': 14, 'virtual_batch_size': 64, 'optimizer_type': 'rmsprop', 'p': 0.19153546053382878}"
      ],
      "metadata": {
        "trusted": true,
        "id": "2SBgZt2ksEqf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros obtenidos del estudio\n",
        "# Parámetros obtenidos del estudio\n",
        "best_params =  hip\n",
        "\n",
        "# Asignación de parámetros\n",
        "n_d = best_params['n_d']\n",
        "n_a = best_params['n_a']\n",
        "n_steps = best_params['n_steps']\n",
        "gamma = best_params['gamma']\n",
        "lambda_sparse = best_params['lambda_sparse']\n",
        "mask_type = best_params['mask_type']\n",
        "batch_size = best_params['batch_size']\n",
        "emb = best_params['emb']\n",
        "p = best_params['p']\n",
        "momentum = best_params['momentum']\n",
        "learning_rate = best_params['learning_rate']\n",
        "weight_decay = best_params['weight_decay']\n",
        "scheduler_gamma = best_params['scheduler_gamma']\n",
        "step_size = best_params['step_size']\n",
        "virtual_batch_size = best_params['virtual_batch_size']\n",
        "optimizer_type = best_params['optimizer_type']\n",
        "\n",
        "if optimizer_type == 'adam':\n",
        "    optimizer_fn = torch.optim.Adam\n",
        "    optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n",
        "elif optimizer_type == 'adamw':\n",
        "    optimizer_fn = torch.optim.AdamW\n",
        "    optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n",
        "elif optimizer_type == 'sgd':\n",
        "    optimizer_fn = torch.optim.SGD\n",
        "    optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "elif optimizer_type == 'rmsprop':\n",
        "    optimizer_fn = torch.optim.RMSprop\n",
        "    optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "\n",
        "# Definición de la técnica de aumento de datos\n",
        "aug = RegressionSMOTE(p=p)\n",
        "\n",
        "# Identificación de columnas categóricas y sus dimensiones\n",
        "cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "cat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\n",
        "cat_emb_dim = [min(emb, (dim + 1) // 2) for dim in cat_dims]\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Valores de lambda_sparse para iterar\n",
        "lambda_values = [lambda_sparse]\n",
        "\n",
        "# Listas para guardar las máscaras y matrices de explicación de cada iteración\n",
        "all_masks = []\n",
        "all_explain_matrices = []\n",
        "\n",
        "for iteration, lambda_sparse in enumerate(lambda_values):\n",
        "    print(f\"Iteración {iteration + 1}, lambda_sparse={lambda_sparse}\")\n",
        "\n",
        "    # Inicialización del modelo TabNetRegressor\n",
        "    clf = TabNetRegressor(\n",
        "        cat_dims=cat_dims,\n",
        "        cat_emb_dim=cat_emb_dim,\n",
        "        cat_idxs=cat_idxs,\n",
        "        n_d=n_d,\n",
        "        n_a=n_a,\n",
        "        n_steps=n_steps,\n",
        "        gamma=gamma,\n",
        "        lambda_sparse=lambda_sparse,\n",
        "        mask_type=mask_type,\n",
        "        optimizer_fn=optimizer_fn,\n",
        "        optimizer_params=optimizer_params,\n",
        "        scheduler_params={\"gamma\": scheduler_gamma, \"step_size\": step_size},\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        momentum=momentum,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    clf.fit(\n",
        "        X_train=X_train,\n",
        "        y_train=y_train[:,0:1],\n",
        "        eval_set=[(X_train, y_train[:,0:1]), (X_valid, y_valid[:,0:1])],\n",
        "        eval_name=['train', 'valid'],\n",
        "        eval_metric=['mae'],\n",
        "        loss_fn=my_r2_score_fn,\n",
        "        max_epochs=200,\n",
        "        patience=60,\n",
        "        batch_size=batch_size,\n",
        "        virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=0,\n",
        "        drop_last=False,\n",
        "        augmentations=aug,\n",
        "    )\n",
        "\n",
        "#     # Generar explicación\n",
        "#     explain_matrix, masks = clf.explain(X, normalize=True)\n",
        "\n",
        "#     # Guardar resultados\n",
        "#     all_masks.append(np.array([masks[i] for i in range(n_steps)]))  # Guardar las máscaras\n",
        "#     all_explain_matrices.append(explain_matrix)  # Guardar explain_matrix como array de NumPy\n",
        "# savepath='/kaggle/working/'\n",
        "# # Guardar las máscaras y matrices de explicación en archivos\n",
        "# for i, (masks, explain_matrix) in enumerate(zip(all_masks, all_explain_matrices)):\n",
        "#     # Guardar las máscaras como un archivo numpy\n",
        "#     np.save(f\"{savepath}masks_iteration_{i+1}.npy\", masks, allow_pickle=True)\n",
        "#     # Guardar la matriz de explicación como un archivo numpy\n",
        "#     np.save(f\"{savepath}explain_matrix_iteration_{i+1}.npy\", explain_matrix,allow_pickle=True)\n",
        "\n",
        "# print(\"¡Proceso completo! Las máscaras y matrices de explicación se han guardado correctamente.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "JsiddWBOsEqf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "y_pred=clf.predict(X_test)\n",
        "# Calcular el R2\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Regression Output 1: True vs Predicted')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r-', label=f'($R^2={r2:.2f}$)')\n",
        "# Mostrar leyenda y gráfico\n",
        "plt.yscale('symlog')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "wjUzz1bCsEqf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "explain_matrix, masks = clf.explain(X, normalize=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cijJuQEksEqf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "min_val = np.min([np.min(masks[i]) for i in range(n_steps)])\n",
        "max_val = np.max([np.max(masks[i]) for i in range(n_steps)])\n",
        "fig, axs = plt.subplots(1, n_steps, figsize=(20,20))\n",
        "# Iteración para mostrar cada máscara\n",
        "for i in range(n_steps):\n",
        "    im = axs[i].imshow(masks[i], aspect='auto', cmap='viridis', vmin=min_val, vmax=max_val)\n",
        "    axs[i].set_title(f\"Mask {i + 1}\", fontsize=16)\n",
        "    axs[i].set_xlabel(\"Features\", fontsize=14)\n",
        "    if i == 0:  # Solo mostrar ejes en la primera subfigura\n",
        "        axs[i].set_ylabel(\"Samples\", fontsize=14)\n",
        "    else:\n",
        "        axs[i].set_yticks([])\n",
        "# Añadir un único colorbar para toda la figura al final\n",
        "cbar = fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.046, pad=-0.22)\n",
        "cbar.set_label(\"Values\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "IBmU9ulisEqf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(22, 15))\n",
        "pd.Series(clf.feature_importances_, index=df.columns).plot.bar(title='Global Feature Importances')"
      ],
      "metadata": {
        "trusted": true,
        "id": "Md4bfjebsEqf"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}